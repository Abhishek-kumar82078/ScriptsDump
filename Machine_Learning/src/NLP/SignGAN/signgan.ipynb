{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "research",
   "display_name": "research"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SignGAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "\n",
    "from bert_utils import Bert\n",
    "from utils.video import Video\n",
    "from utils.conv_attention import *\n",
    "from utils.generator import *\n",
    "from utils.discriminator import *\n",
    "from utils.losses import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.compat.v1.Session(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretrained BERT Model\n",
    "Multilingual Cased BERT is used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Done loading 196 BERT weights from: models/multi_cased_L-12_H-768_A-12/bert_model.ckpt into <bert.model.BertModelLayer object at 0x000002ED2903A888> (prefix:bert). Count of weights not found in the checkpoint was: [0]. Count of weights with mismatched shape: [0]\nUnused weights from checkpoint: \n\tbert/embeddings/token_type_embeddings\n\tbert/pooler/dense/bias\n\tbert/pooler/dense/kernel\n\tcls/predictions/output_bias\n\tcls/predictions/transform/LayerNorm/beta\n\tcls/predictions/transform/LayerNorm/gamma\n\tcls/predictions/transform/dense/bias\n\tcls/predictions/transform/dense/kernel\n\tcls/seq_relationship/output_bias\n\tcls/seq_relationship/output_weights\n(1, 64, 768) (1, 768)\n"
    }
   ],
   "source": [
    "bert = Bert()\n",
    "word_embeddings, sentence_embeddings = bert.predict(['sonst wechselhaft mit schauern und gewittern die uns auch am wochenende begleiten'])\n",
    "print(word_embeddings.shape, sentence_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clips = 32\n",
    "T = 16 # let\n",
    "MAX_VIDEO_LENGTH = 512      # 475 is the longest\n",
    "FRAME_DIM = (64, 64, 3)\n",
    "VIDEO_DIM = (512, 64, 64, 3)\n",
    "data_dir = 'phoenix-2014-T.v3/PHOENIX-2014-T-release-v3/PHOENIX-2014-T/features/fullFrame-210x260px/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video Object\n",
    "with example from training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(640, 64, 64, 3)\n(40, 16, 64, 64, 3)\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'\\npad_mask = video_obj.padding_mask(current_sequence_length)\\nprint(pad_mask.shape)\\n\\nlook_mask = video_obj.look_ahead_mask()\\nprint(look_mask.shape)\\n'"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "video_obj = Video(T, MAX_VIDEO_LENGTH, FRAME_DIM, VIDEO_DIM, data_dir)\n",
    "\n",
    "video_real = video_obj.get_video('train', '05January_2010_Tuesday_tagesschau-2664')\n",
    "current_sequence_length = video_real.shape[0]\n",
    "\n",
    "video_real = video_obj.preprocess_video(video_real)\n",
    "print(video_real.shape)\n",
    "\n",
    "video_real = video_obj.divide_sequence(video_real)\n",
    "print(video_real.shape)\n",
    "\n",
    "video_wrong = video_obj.get_video('train', '03June_2011_Friday_tagesschau-7649')\n",
    "current_sequence_length = video_wrong.shape[0]\n",
    "\n",
    "video_wrong = video_obj.preprocess_video(video_wrong)\n",
    "print(video_wrong.shape)\n",
    "\n",
    "video_wrong = video_obj.divide_sequence(video_wrong)\n",
    "print(video_wrong.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention\n",
    "* First, perform convolutions across the whole video, then go for separable self attention for the whole video masked, i.e, (40, 40T, H, W, C), outputs (40, 40T, H, W, C), where T=16 and 40=640//T. 640 being the length of the whole video.\n",
    "* Convolution operations will be restricted to only T frames at a time. There will be no intermingling of 2 or more sets of T frames. Thus 3D CONV will take care of extracting local, temporal and spatial features only. Hack : Batched\n",
    "* After the conv operations there will be MAX_SEQ_LENGTH // T i.e, 40 (here) elements each of size (T, H, W, C), making the output of convolution ops, to be of dim -> (40, T, H, W, C) converted to (40T, H, W, C). This will be passed through attention blocks and outputs (40, 40T, H, W, C) along with masking.\n",
    "* This attention performs masked attention. We will have MAX_SEQ_LENGTH // T i.e, 40 (here) masks in total each for time, height, width. Each mask being of shape (40, 40T, H, W, H*W) for time, W*T for height and H*T for width.\n",
    "* Only after the whole video is generated are the losses calculated, and backpropped.\n",
    "* During testing just \"start\" token will be provided and the rest of the sequence will be just padding, and each time the generator produces T frames, those T frames will be concatenated along with the \"start\" token and then convoluted again to produce the next T frames.\n",
    "\n",
    "### Attention mechanism will require residual connections otherwise gradients will vanish\n",
    "\n",
    "## Word Frame Attention\n",
    "* Last dimension of both masked_attention_output and semantic_word_matrix must be same\n",
    "* 2nd last dimenstion i.e, 1st dimenstion of semantic_word_matrix should equal to H*W of masked-separable-self-attention output \n",
    "* that is, we bring both to a common semantic space using conv for frames and dense for embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(4, 16, 8, 8, 3)\n"
    }
   ],
   "source": [
    "attention = Attention(channel_dimension=3)\n",
    "\n",
    "num_clips = 32\n",
    "t, h, w, c = 16, 8, 8, 3\n",
    "\n",
    "x = tf.random.normal(shape=(t, h, w, c), mean=0.5, stddev=0.5)\n",
    "\n",
    "# this step is done only before the 1st attention block\n",
    "x = tf.reshape(tf.repeat(x, repeats=num_clips, axis=0), (num_clips, t, h, w, c))\n",
    "\n",
    "mask_t = look_ahead_mask(num_clips, (t, h, w, h*w))\n",
    "mask_h = look_ahead_mask(num_clips, (t, h, w, t*w))\n",
    "mask_w = look_ahead_mask(num_clips, (t, h, w, t*h))\n",
    "\n",
    "word_embeddings = tf.squeeze(word_embeddings)\n",
    "\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conv-Attention Block\n",
    "* The input (640, 64, 64, 3) video will be explicitly split into (40, 16, 64, 64, 3).\n",
    "* HACK : This reshaped input will be fed to 3D conv block with batch size being 40(hack), for local spatial and temporal feature extraction\n",
    "* Conv Block output will be (40, 16, 8, 8, 64), which will be reshaped to (640, 8, 8, 64) and sent forward for masked-separable-self-attention followed by word-frame-attention for a few number of times (Attention Block), with addition and layer normalisation after each attention block\n",
    "* Conv Block : \n",
    "    * Format : num_filters, kernel, strides, padding\n",
    "    * {8, (3, 3, 3), (2, 2, 2), same} -> {16, (3, 3, 3), (2, 2, 2), same} -> {32, (3, 3, 3), (2, 2, 2), same} -> {out_channels(64), (3, 3, 3), (2, 2, 2), same}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Overall Video :  (64, 64, 64, 3)\nInput Video Shape :  (4, 16, 64, 64, 3)\nAttention Output Shape :  (4, 8, 8, 8, 64)\n"
    }
   ],
   "source": [
    "num_clips = 4   # instead of 32\n",
    "t, h, w, c = 16, 64, 64, 3\n",
    "print(\"Overall Video : \", (num_clips * t, h, w, c))\n",
    "print(\"Input Video Shape : \", (num_clips, t, h, w, c))\n",
    "\n",
    "x = tf.random.normal(shape=(num_clips, t, h, w, c), mean=0.5, stddev=0.5)\n",
    "word_embeddings = tf.squeeze(word_embeddings)\n",
    "\n",
    "conv_attn = ConvAttn(num_attention_blocks=4)\n",
    "x = conv_attn(x, word_embeddings)\n",
    "print(\"Attention Output Shape : \", x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conditional GAN (3D)\n",
    "* The conditions generated by ConvAttn block will be used along ith upscaled randomness to generate a video with T frames for each batch of size num_clips(40)\n",
    "* use LayerNorm instead of BatchNorm, because BatchNorm outputs NaN because of the padding and masking\n",
    "* In the future if this fails, even this block may have attention blocks in the intermediate layers\n",
    "* Upsampling z : \n",
    "* Concatenate upsampled z with conv_attn_output\n",
    "* upsample to produce the video\n",
    "* this 'z' will be upscaled to num_clips x (num_clips x t, h, w, channels) and concatenated with conv_attn_output, along the channel dimension, which together will be upscaled using deconv to produce a 40 x (16, 64, 64, 3) video. Hack : Use 40 as batch size throughout\n",
    "* We will feed 'z' from outside. If it is inside it'll stay constant and won't be random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Output Shape :  (4, 16, 64, 64, 3)\nOutput Video Shape :  (64, 64, 64, 3)\n"
    }
   ],
   "source": [
    "z = tf.random.normal(shape=(1, 100))\n",
    "cdcgan = CDCGAN()\n",
    "z = cdcgan(z, x)\n",
    "print(\"Output Shape : \", z.shape)\n",
    "print(\"Output Video Shape : \", (z.shape[0] * z.shape[1], z.shape[2], z.shape[3], z.shape[4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus output video is in the shape we wanted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full Generator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(64, 768)\nConv1 Out Shape :  (32, 16, 64, 64, 8)\nConv2 Out Shape :  (32, 8, 32, 32, 16)\nConv3 Out Shape :  (32, 4, 16, 16, 32)\nConv4 Out Shape :  (32, 2, 8, 8, 64)\n(32, 8, 8, 8, 256) (32, 8, 8, 8, 512)\n(32, 16, 64, 64, 3)\n"
    }
   ],
   "source": [
    "num_clips = 32   # instead of 32\n",
    "t, h, w, c = 16, 64, 64, 3\n",
    "\n",
    "x = tf.random.normal(shape=(num_clips, t, h, w, c), mean=0.5, stddev=0.5)\n",
    "word_embeddings = tf.squeeze(word_embeddings)\n",
    "z = tf.random.normal(shape=(1, 100))\n",
    "\n",
    "generator = Generator()\n",
    "x = generator(x, word_embeddings, z)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discriminators\n",
    "* Batched\n",
    "* For discriminators we also consider the batch dimension\n",
    "* Before the video goes into the discriminator we have to reshape the video to (1, num_clips * t, h, w, c), 1 -> batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video Discriminator\n",
    "* Gotta setup proper input pipelines for discriminator training\n",
    "* 64, 64, 64, 3 -> 32, 64, 64, 16 -> 16, 64, 64, 32 -> 8, 32, 32, 64 -> 4, 16, 16, 128 -> 2, 8, 8, 256 -> 1, 4, 4, 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tf.Tensor(\n[[0.45877308]\n [0.6192273 ]], shape=(2, 1), dtype=float32)\n"
    }
   ],
   "source": [
    "v = tf.random.normal((2, 32 * 16, 64, 64, 3))   # batch dimension considered \n",
    "s = tf.random.normal((2, 768))\n",
    "\n",
    "video_disc = VideoDiscriminator()\n",
    "vid_disc_out = video_disc(v, s)\n",
    "print(vid_disc_out)     # 2 outputs for batch_size = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frame Discriminator\n",
    "* 2-fold\n",
    "* Outputs \"{0 ... 1}\" for single frame level\n",
    "* Outputs temporal (difference between 2 consecutive frames in euclidean norm, for each pair) downscaled as output, i.e, 1 number as output per pair of consecutive frames\n",
    "* One part of both the discriminators are shared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(2, 8) (2, 7)\n"
    }
   ],
   "source": [
    "# Total frames kept 8 instead of 640 because of ResourceExhaustError\n",
    "v = tf.random.normal((2, 8, 64, 64, 3))    # kept batch_size = 2 here\n",
    "s = tf.random.normal((2, 768))\n",
    "\n",
    "frame_disc = FrameDiscriminator()\n",
    "frame_disc_out, motion_disc_out = frame_disc(v, s)\n",
    "\n",
    "print(frame_disc_out.shape, motion_disc_out.shape)\n",
    "# Thus 2 outputs of each frame and motion disc, for batch_size=2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = Discriminator()\n",
    "\n",
    "v = tf.random.normal((1, 32 * 16, 64, 64, 3))   # batch dimension considered \n",
    "s = tf.random.normal((1, 768))                  # keeping batch_size = 1\n",
    "\n",
    "video_disc_out, frame_disc_out, motion_disc_out = discriminator(v, s)\n",
    "print(video_disc_out.shape, frame_disc_out.shape, motion_disc_out.shape)\n",
    "print(discriminator.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Losses\n",
    "* Matching Aware Losses\n",
    "* Output of motion discriminator is a bit high in value (though only used for the generator)\n",
    "* Using Scheme 2 of Microsoft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tf.Tensor(0.7362369, shape=(), dtype=float32)\ntf.Tensor(0.7079885, shape=(), dtype=float32)\ntf.Tensor(0.68915033, shape=(), dtype=float32)\n"
    }
   ],
   "source": [
    "# Doing with the same ones, but won't be the case\n",
    "vid_loss = video_loss(vid_disc_out, vid_disc_out, vid_disc_out)\n",
    "print(vid_loss)\n",
    "fr_loss = frame_loss(frame_disc_out, frame_disc_out, frame_disc_out)\n",
    "print(fr_loss)\n",
    "mot_loss = motion_loss(motion_disc_out, motion_disc_out, motion_disc_out)\n",
    "print(mot_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tf.Tensor(0.67633134, shape=(), dtype=float32)\ntf.Tensor(0.20451142, shape=(), dtype=float32)\n"
    }
   ],
   "source": [
    "print(discriminator_loss(vid_loss, fr_loss, mot_loss))\n",
    "print(generator_loss(vid_disc_out, frame_disc_out, motion_disc_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes correct video, wrong video and word embeddings, sentence\n",
    "# all of this must be preprocessed (padded and stuff)\n",
    "# Video must be explicitly divided into T frames\n",
    "def train_step(video_real, video_wrong, w, s):\n",
    "    num_clips, t, h, w, c = video_real.shape\n",
    "\n",
    "    w = tf.squeeze(w)\n",
    "    z = tf.random.normal(shape=(1, 100))\n",
    "    \n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        video_fake = generator(video_real, w, z)\n",
    "\n",
    "        # All frames put together with bs = 1\n",
    "        video_real = tf.reshape(video_real, (1, num_clips * t, h, w, c))\n",
    "        video_wrong = tf.reshape(video_wrong, (1, num_clips * t, h, w, c))\n",
    "        video_fake = tf.reshape(video_fake, (1, num_clips * t, h, w, c))\n",
    "\n",
    "        # Discriminator out\n",
    "        disc_video_real, disc_frame_real, disc_motion_real = discriminator(video_real, s)\n",
    "        disc_video_wrong, disc_frame_wrong, disc_motion_wrong = discriminator(video_wrong, s)\n",
    "        disc_video_fake, disc_frame_fake, disc_motion_fake = discriminator(video_fake, s)\n",
    "\n",
    "        # Losses\n",
    "        total_video_loss = video_loss(disc_video_real, disc_video_wrong, disc_video_fake)\n",
    "        total_frame_loss = frame_loss(disc_frame_real, disc_frame_wrong, disc_frame_fake)\n",
    "        total_motion_loss = motion_loss(disc_motion_real, disc_motion_wrong, disc_motion_fake)\n",
    "\n",
    "        disc_loss = discriminator_loss(total_video_loss, total_frame_loss, total_motion_loss)\n",
    "        gen_loss = generator_loss(disc_video_fake, disc_frame_fake, disc_motion_fake)\n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizers and Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator()\n",
    "discriminator = Discriminator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0002  # Following microsoft\n",
    "generator_optimizer = tf.keras.optimizers.Adam(learning_rate)     # rest default\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(learning_rate)     # rest default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                                 discriminator_optimizer=discriminator_optimizer,\n",
    "                                 generator=generator,\n",
    "                                 discriminator=discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_step(video_real[:8], video_wrong[:8], word_embeddings, sentence_embeddings)"
   ]
  }
 ]
}